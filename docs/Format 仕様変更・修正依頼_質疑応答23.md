# 仕様変更・修正依頼 質疑応答23

---

## 解決済み項目（質疑応答22から）

| 項目 | 対応内容 | ステータス |
|------|----------|-----------|
| P106 | 言語設定のlocalStorage保存 | ✅ 解決 |
| P107 | プレビュードラッグ（パン移動）実装 | ✅ 解決 |
| P108 | PPTスライドUI（◀▶ボタン、Goボタン追加） | ✅ 解決 |
| P109 | テキスト領域番号表示（色・透過率調整） | ✅ 解決 |
| P110 | PPT 14→1スライド問題（nullishチェック修正） | ✅ 解決 |
| P111 | PPT並列プレビュー＋ベンチマーク機能 | ✅ 解決 |

---

## 実装済み項目（質疑応答23）

| 項目 | 対応内容 | ステータス |
|------|----------|-----------|
| P112 | NLLBのCPUフォールバック機能実装 | ✅ 解決 |
| P113 | VAD/WhisperのCPUフォールバック確認・実装 | ✅ 解決 |
| P114 | プレビュー/テキスト領域リサイズ機能実装（ドラッグ＋localStorage保存） | ✅ 解決 |
| P115 | PPTベンチマーク二分探索実装 | ✅ 解決 |
| P116 | PowerPoint白いウィンドウ非表示化 → Visible=True+最小化に変更 | ✅ 解決 |
| P117 | pywintypesモジュールエラー（pywin32再インストールで解決） | ✅ 解決 |
| P118 | 二分探索アルゴリズム実装（P115と統合） | ✅ 解決 |
| Q65 | NLLB/VAD/Whisper個別GPU設定機能追加 | ✅ 解決 |
| Q66 | PPTプレビュー表示修正（P116変更を元に戻し） | ✅ 解決 |
| Q68 | PPTプレビュー一括出力＋キャッシュ機能実装 | ✅ 解決 |

---

## 新規問題・要望（未解決）

### P119残存: Word/Excel読み込み問題

**報告日:** 2025-12-31

**現象:**
1. Word読み込み時エラー: `TypeError: 'NoneType' object cannot be converted to 'PyDict'`
   - テストファイル: `docs/Sample Data/JP02.docx`
2. Excel複数シートが読み込めず1シートのみ表示
   - テストファイル: `docs/Sample Data/Page_Confirmation.xlsx`

**根本原因（調査済み）:**
- Word: `extract_word`で位置情報が取得できない場合に`position=None`を設定
- Rust側（`python/mod.rs:562-563`）で`None`を`PyDict`にキャスト失敗

**対応予定:**
- Python側で`position=None`を`position={}`に変更

---

### P125追加: 翻訳スキップ対象のキャッシュミス除外

**報告日:** 2025-12-31

**現象:**
- 数字・記号等の翻訳スキップ対象がキャッシュミスとして判定される
- 翻訳が不要なのにモデルがロードされてしまう

**期待動作:**
- スキップ対象を除外した上で「翻訳が必要な文があり、且つオフライン翻訳の時」のみモデルロード

**対応予定:**
- Rust側でスキップ判定を追加
- スキップ対象はキャッシュミス判定から除外

---

## 過去の要望（対応済み）

P117
.txt, .md, ワードファイル、エクセルファイルの文字列の位置をプレビュー画面と合わせる方法をChatGPTに聞くので、非常に丁寧且つ詳細なプロンプトを"C:\Terunslator_Windows_Vue_AutoPDCA\docs\Prompt for Sentence_Position_02.md"に作成して。
→ プロンプト作成済み（ChatGPTへ）

---

## Claudeからの質問・コメント

---

### Q56. P112 NLLBのCPUフォールバック

**現状確認:**
NLLBはCTranslate2を使用しています。CTranslate2はGPU/CPU両対応ですが、現在の実装ではdevice選択の明示的なフォールバック処理がない可能性があります。

**質問1:** CPUフォールバックが必要なのはどの状況ですか？

- [x] A) GPUメモリ不足時に自動でCPUに切り替え
- [x] B) GPU未搭載PCでも動作するように
- [x] C) 設定画面でGPU/CPU選択を可能に
- [ ] D) その他（下記に記載）

→ ユーザー回答:

**質問2:** CPUフォールバック時の動作について

- [x] A) 自動切り替え（エラー時にCPUで再試行）
- [x] B) 起動時にGPU利用可否をチェックし、不可ならCPUモードで起動
- [ ] C) ユーザーに通知してからCPUに切り替え
- [ ] D) その他

→ ユーザー回答:

---

### Q57. P113 VAD/WhisperのCPUフォールバック確認

**調査結果:**

| コンポーネント | GPU対応 | CPUフォールバック | 備考 |
|---------------|---------|------------------|------|
| Silero VAD | ✅ | ✅ 実装済み | torch.device自動選択 |
| Whisper | ✅ | ⚠️ 要確認 | device指定あり |

**質問:** Whisperで「GPUメモリ不足→CPU」の自動フォールバックは現在動作していますか？

- [ ] A) 動作している
- [ ] B) 動作していない
- [x] C) わからない/テストしていない

→ ユーザー回答:

---

### Q58. P114 プレビュー/テキスト領域のリサイズ

**質問1:** リサイズ方式の希望は？

- [x] A) ドラッグでリサイズ（中央の境界線をドラッグ）
- [ ] B) 固定比率選択（50:50 / 30:70 / 70:30 等のボタン）
- [ ] C) 数値入力（左パネル幅をピクセルまたは%で指定）
- [ ] D) A + B 両対応

→ ユーザー回答:

**質問2:** リサイズ設定の保存は？

- [x] A) はい、localStorageに保存して次回起動時に復元
- [ ] B) いいえ、毎回デフォルトに戻る

→ ユーザー回答:

---

### Q59. P115 PPTベンチマーク改善

**現状:** P111で実装したベンチマーク機能は、現在開いているPPTファイルを使用して2/4/6/8並列をテストしています。

**提案仕様:**
1. 組み込みベンチマークファイル（Benchmark_01.pptx）を使用
2. テスト項目:
   - 10ページ分プレビュー生成
   - 20ページ分プレビュー生成
   - 全ページプレビュー生成
3. 各ページ数で最適な並列数を探索
4. 結果に基づいて「最適設定」を自動適用

**質問1:** ベンチマークファイルのサイズ/スライド数は？

→ ユーザー回答: 824KB, 65スライド

**質問2:** ベンチマーク結果の表示形式は？

- [ ] A) 進捗表をMarkdown表形式で表示
- [x] B) グラフ表示（横軸: 並列数、縦軸: 処理時間）
- [ ] C) テキストログのみ
- [ ] D) A + B

→ ユーザー回答:

**質問3:** 最適設定の適用方法は？

- [ ] A) 自動適用（localStorageに保存）
- [x] B) ユーザーに確認してから適用
- [ ] C) 結果表示のみ（適用はユーザーが手動で）

→ ユーザー回答:

---

### Q60. P116 白いウィンドウ問題

**現状分析:**
プレビュー生成時にPowerPoint COMを使用しており、サブプロセスで実行しています。白いウィンドウはPowerPointアプリケーションが一時的に表示されている可能性があります。

**質問1:** 白いウィンドウの状態を教えてください。

- [x] A) PowerPointのウィンドウが一瞬見える
- [ ] B) 無題のウィンドウが開く（タイトルバーなし）
- [ ] C) タスクバーにアイコンが増える
- [ ] D) その他（下記に記載）

→ ユーザー回答:

**質問2:** 白いウィンドウは消えますか？

- [x] A) 自動で消える（プレビュー完了後）
- [ ] B) 残り続ける（手動で閉じる必要あり）
- [ ] C) 不定期に残る

→ ユーザー回答:

---

### Q61. P117 テキスト位置合わせプロンプト

**確認:** ChatGPT用プロンプトを作成します。以下の情報を含めてよいですか？

- [ ] A) はい、プロジェクト全体の構成を含める
- [x] B) いいえ、関連ファイルのみに限定
- [ ] C) その他

**対象ファイル形式:**
- txt: テキストファイル（行ベース）
- md: Markdownファイル（見出し/段落/コード等）
- Word: docx（段落/表/オートシェイプ）
- Excel: xlsx（セル/オートシェイプ）

→ ユーザー回答:

---

### Q62. P118 分割探索アルゴリズム最適化

**現状:** P115のベンチマークで並列数を探索していますが、最適化手法について詳細を確認させてください。

**質問1:** 「分割探索」とは何を指していますか？

- [x] A) PPTプレビューの並列数（2/4/6/8...）の最適値探索
- [ ] B) バッチ翻訳のチャンクサイズ最適化
- [ ] C) ドキュメント分割（文/段落/ページ）の最適化
- [ ] D) その他（下記に記載）

→ ユーザー回答:

**質問2:** 最適化の目的は？

- [x] A) 速度最優先（最短処理時間）
- [ ] B) メモリ効率優先（低VRAMでも動作）
- [ ] C) 速度とメモリのバランス
- [ ] D) 成功率優先（エラーを避ける）

→ ユーザー回答:

**質問3:** 探索アルゴリズムの希望は？

- [ ] A) 全探索（全パターンをテスト）
- [x] B) 二分探索（最適値を効率的に絞り込む）
- [ ] C) 黄金分割探索（1次元最適化）
- [ ] D) 適応的探索（過去の結果から学習）
- [ ] E) その他

→ ユーザー回答:

---

### Q63. P116 白いウィンドウ非表示化

**提案:** PowerPoint COMの`Visible`プロパティを`False`に設定することで、白いウィンドウを非表示にできます。

- [x] A) はい、非表示にしてください
- [ ] B) いいえ、現状維持（進捗確認のため表示を残す）

→ ユーザー回答:

---

### Q64. P115/P118 二分探索の範囲

**質問:** 二分探索の探索範囲（並列数の最小値・最大値）は？

- [ ] A) 1〜16（CPUコア数に依存しない固定範囲）
- [x] B) 1〜CPUコア数（動的に決定）
- [ ] C) 2〜8（現在のベンチマーク範囲）
- [ ] D) その他（下記に記載）

→ ユーザー回答:

---

### Q65. P112/P113: GPU設定が反映されない問題

**ユーザー報告:**
設定タブで「GPUアクセラレーションを使用する」のチェックを外しているのに、NLLBモデル/VAD/Whisperをロードすると、GPU専用メモリが使われる。

**質問1:** 設定変更後、アプリを再起動しましたか？

- [ ] A) はい、再起動した
- [x] B) いいえ、再起動していない

→ ユーザー回答:

**質問2:** 設定タブの「GPUアクセラレーション」設定は、どのモジュールに影響すべきですか？

- [ ] A) NLLB（オフライン翻訳）のみ
- [ ] B) VAD/Whisper（音声認識）のみ
- [ ] C) 両方（NLLB + VAD/Whisper）
- [x] D) その他

→ ユーザー回答:A,B個別に設定したい。

---

### Q66. P115/P116/P118: PPTプレビューが表示されない問題

**ユーザー報告:**
PPTファイルをD&Dすると、imageDataが空（0バイト）になり、プレビューが表示されない。

**ログ:**
```
[P78B] loadPreview: received result: {imageLength: 0, ...}
[P78B] loadPreview: imageData is empty, setting previewImage to null
```

**質問1:** P116の変更（`Visible=False`）を一時的に元に戻して確認してよいですか？

- [x] A) はい、元に戻して確認してください
- [ ] B) いいえ、別の方法を試してください

→ ユーザー回答:

**質問2:** 以前（P116変更前）はPPTプレビューは正常に表示されていましたか？

- [x] A) はい、正常に表示されていた
- [ ] B) いいえ、以前から問題があった
- [ ] C) わからない

→ ユーザー回答:

---

### Q67. P117: pywintypesモジュールエラー

**ユーザー報告:**
Excelファイル読み込み時に `No module named 'pywintypes'` エラー。

**確認コマンド:** 以下を実行して結果を教えてください:

```cmd
C:\Terunslator_Windows_Vue_AutoPDCA\python\.conda_env\python.exe -c "import win32com.client; print('OK')"
```

→ ユーザー回答（コマンド実行結果）:C:\Terunslator_Windows_Vue_AutoPDCA>C:\Terunslator_Windows_Vue_AutoPDCA\python\.conda_env\python.exe -c "import win32com.client; print('OK')"
OK

C:\Terunslator_Windows_Vue_AutoPDCA>

**質問:** pywin32を再インストールしてよいですか？

- [x] A) はい、再インストールしてください
- [ ] B) いいえ、別の方法を試してください

→ ユーザー回答:

---

### Q68. PPTプレビュー一括出力への変更

**ユーザー提案:**
PPTのプレビューを取得する際、1ページずつ開いて閉じてをやり直すのではなく、1度開いた時点で全てのページを取得できないか？

**現状（非効率）:**
```
1ページ目: PowerPoint起動 → ファイル開く → 1ページ出力 → ファイル閉じる → PowerPoint終了
2ページ目: PowerPoint起動 → ファイル開く → 2ページ出力 → ファイル閉じる → PowerPoint終了
...（65ページなら65回繰り返し）
```

**提案（効率的）:**
```
PowerPoint起動 → ファイル開く → 全ページ一括出力 → ファイル閉じる → PowerPoint終了
```

**メリット:**
- PowerPoint起動は1回のみ（COM初期化に1-2秒かかるのを節約）
- ファイルオープンは1回のみ
- 処理時間: 65ページでも数秒で完了

**質問:** この効率的な方式に変更してよいですか？

- [x] A) はい、一括出力方式に変更してください
- [ ] B) いいえ、現状維持

→ ユーザー回答:

---

## 追加質問（セッション2）

---

### Q69. P112/P113/Q65: GPU設定が反映されない問題（詳細調査）

**ユーザー報告:**
設定タブで「GPUアクセラレーションを使用する」のチェックを外しても:
- NLLBロードボタン押下 → GPU専用メモリにロードされる
- 音声認識/同時翻訳タブ表示 → Whisper/VADがGPUにロードされる

**調査ポイント:**

**質問1:** 設定タブの「GPUアクセラレーション」設定は、どこに保存されていますか？

- [ ] A) localStorage
- [ ] B) Tauriの設定ファイル
- [x] C) わからない

→ ユーザー回答:実装を確認してください。

**Claude調査結果:**
```
Vue: localSettings.use_gpu → Pinia Store → Rust save_settings → データベース保存
```
設定は保存されているが、**Python側に伝達されていない**。

**質問2:** Python側の設定APIとRust/Vue側の設定が連携していない可能性があります。現在の実装フローを確認させてください:

1. Vue設定画面 → Rust Command → Python API の経路が必要
2. 現在は Python側に `set_nllb_device_setting()` / `set_whisper_device_setting()` を追加済み
3. **問題:** Rust Command とVue側の呼び出しがまだ未実装

この理解で正しいですか？

- [ ] A) はい、Rust/Vue側の連携が必要
- [ ] B) いいえ、別の問題がある
- [x] C) わからない

→ ユーザー回答:実装を確認してください。

**Claude調査結果:**
| 層 | NLLB | Whisper |
|----|------|---------|
| Python | ✅ `set_nllb_device_setting()` 実装済み | ✅ `set_whisper_device_setting()` 実装済み |
| Rust | ✅ `set_nllb_device` コマンド実装済み | ❌ **未実装** |
| Vue | ❌ **呼び出し未実装** | ❌ **呼び出し未実装** |

**問題点:**
1. Vue側で `use_gpu` チェックボックス変更時にRustコマンドを呼んでいない
2. Whisper用のRustコマンドが未実装
3. 現在の `use_gpu` は単一設定だが、ユーザーはNLLB/Whisper個別設定を希望

**修正方針:**
1. Rust: `set_whisper_device` コマンド追加
2. Vue: GPU設定セクションをNLLB/Whisper個別チェックボックスに変更
3. Vue: チェックボックス変更時に即座にRustコマンド呼び出し

---

### Q70. Q68: PPTプレビュー一括出力が機能していない問題

**ユーザー報告:**
まだプレビューを1ページ追加する度に、PPTが開いたり閉じたりする。

**調査ポイント:**

**質問1:** プレビュー呼び出しの経路を確認させてください:

現在の実装:
- `document.py` の `_get_powerpoint_preview()` にキャッシュ機能を追加
- ただし、呼び出し元が `preview_worker.py`（サブプロセス）の場合、キャッシュは共有されない

**仮説:** `preview_worker.py` が毎回新しいプロセスとして起動されているため、キャッシュが効いていない可能性があります。

**Claude調査結果:**
```
Vue → Rust get_document_preview → Python get_document_preview()
  → _get_office_preview_subprocess()
  → subprocess.run(preview_worker.py) ← 毎回新規プロセス起動！
```

**原因特定:** `_get_office_preview_subprocess()` が毎回 `preview_worker.py` を別プロセスで起動。
前回追加したキャッシュは `document.py` のメモリ上にあるが、サブプロセスには引き継がれない。

この場合の解決策:
- A) `preview_worker.py` 自体に一括出力機能を追加（サブプロセス内で完結）
- B) Rust側からPythonを直接呼び出す方式に変更
- C) ファイルシステムベースのキャッシュ（一時フォルダに全スライドを保存）

どの方式が良いですか？

- [x] A) preview_worker.py に一括出力機能を追加
- [ ] B) Rust側からPythonを直接呼び出す
- [ ] C) ファイルシステムベースのキャッシュ
- [ ] D) その他

→ ユーザー回答:それぞれのメリットデメリット推奨を教えて。

**Claude回答: 各方式のメリット・デメリット比較**

| 方式 | メリット | デメリット | 実装難易度 |
|------|----------|------------|------------|
| **A) preview_worker.py一括** | ・1回のPowerPoint起動で全スライド出力<br>・既存アーキテクチャ維持<br>・COM初期化問題を回避 | ・サブプロセス起動自体のオーバーヘッド（約1秒）<br>・一括出力完了まで待機必要 | ★★☆ 中 |
| **B) Rust直接呼び出し** | ・サブプロセスオーバーヘッドなし<br>・メモリ内キャッシュ共有可能 | ・PyO3/COM/スレッド問題が再発するリスク高<br>・過去にこの問題でサブプロセス方式に変更した経緯あり | ★★★ 高（リスク大） |
| **C) ファイルキャッシュ** | ・プロセス間でキャッシュ共有<br>・アプリ再起動後もキャッシュ有効<br>・段階的取得可能 | ・ディスクI/Oオーバーヘッド<br>・キャッシュクリーンアップ管理が必要<br>・毎回PowerPoint起動は変わらない | ★★☆ 中 |

**🌟 推奨: A) preview_worker.py一括出力**

理由:
1. ユーザー希望が「ファイルD&D時に全スライド一括取得」と合致
2. PowerPoint起動→全スライド出力→終了 を1回で完結
3. 65スライドでも数秒で完了（現状: 65回×2秒 = 130秒 → 改善後: 約5秒）
4. 既存のサブプロセス方式を維持しつつ、引数で「一括モード」を追加するだけ

**質問2:** PPTプレビューの呼び出しタイミングを教えてください:

- [x] A) ファイルD&D時に全スライドを一括取得したい
- [ ] B) スライド切り替え時に1枚ずつ取得（ただし2回目以降はキャッシュ）
- [ ] C) その他

→ ユーザー回答:

---

### Q71. Q70の方式選択確認

推奨方式Aで実装してよいですか？

- [x] A) はい、preview_worker.py一括出力で実装
- [ ] B) いいえ、C)ファイルキャッシュ方式を希望
- [ ] C) その他

→ ユーザー回答:

---

### Q72. Q69のUI確認

GPU設定を個別化する際のUI案:

**案1: 単純分離**
```
□ NLLBにGPUを使用する
□ Whisper/VADにGPUを使用する
```

**案2: 3択セレクト**
```
NLLB: [auto ▼] (auto/GPU/CPU)
Whisper: [auto ▼] (auto/GPU/CPU)
```
※ auto = GPUがあれば使用、なければCPU

どちらが良いですか？

- [ ] A) 案1: チェックボックス（シンプル）
- [x] B) 案2: セレクト（auto選択可能）

→ ユーザー回答:

---

## ユーザー追加コメント

（自由記述欄）



---

## 追加質問（セッション3）

---

### Q73. Q68: PPTバッチエクスポートが動作しない原因特定

**ユーザー報告:**
Q70/Q71の実装後も、まだプレビューを1ページ追加する度にPPTが開いたり閉じたりする。

**Claude調査結果:**

`document.py`の`export_ppt_batch_preview()`関数（2054行目）で問題発見:

```python
# 現在のコード（問題あり）
python_exe = sys.executable or "python"
```

**問題:** PyO3/Tauri内では`sys.executable`がTauri実行ファイルのパスを返すため、Pythonではなくバッチエクスポートが失敗している。

**正しいコード:**（`_get_office_preview_subprocess()`で使用）
```python
python_exe = r"C:\Terunslator_Windows_Vue_AutoPDCA\python\.conda_env\python.exe"
```

**修正内容:**
1. `export_ppt_batch_preview()`内のPythonパスを正しいconda envパスに修正

この修正でよいですか？

- [x] A) はい、修正してください
- [ ] B) いいえ、別の方法を試してください

→ ユーザー回答: A

---

### Q74. P119: 全ドキュメント種類の一括ページ読み込み

**ユーザー要望:**
パワーポイント以外の全てのドキュメントも同じ様に最初に全てのページを読み込んでから処理する様にしてください。

**現状分析:**

| ファイル形式 | 現在の動作 | 一括読み込み対応 |
|-------------|-----------|-----------------|
| PPT/PPTX | ✅ バッチエクスポート（Q70/Q73で実装） | ✅ 完了 |
| PDF | 1ページずつPyMuPDFでレンダリング | ❌ 未対応 |
| Word (DOC/DOCX) | 1ページずつCOM→PDF→レンダリング | ❌ 未対応 |
| Excel (XLS/XLSX) | シート単位でCOM CopyPicture | ❌ 未対応 |
| TXT/MD | 全体を1画像として生成 | N/A（単一ページ） |

**質問1:** 対象ファイル形式を確認させてください:

- [x] A) PDF + Word + Excel（全Office形式）
- [ ] B) PDFのみ（Word/Excelは現状維持）
- [ ] C) Word + Excelのみ（PDFは現状維持）
- [x] D) その他（下記に記載）

→ ユーザー回答:TXT/MD含む上記ファイル形式全て。

**質問2:** 各形式の一括読み込み方式について:

**PDF:**

- 方式: PyMuPDFで全ページを一括レンダリング → ファイルキャッシュ保存
- メリット: COM不要、高速
- デメリット: 大きなPDFはメモリ消費大

**Word:**

- 方式: COM→PDF変換→全ページレンダリング→ファイルキャッシュ
- メリット: PPTと同じパターン
- デメリット: COM初期化に時間がかかる

**Excel:**

- 方式: 全シートを一括CopyPicture→ファイルキャッシュ
- メリット: 高速
- デメリット: シート数が多いとメモリ消費

この方式で進めてよいですか？

- [x] A) はい、全て一括読み込みに変更
- [ ] B) いいえ、特定の形式のみ
- [ ] C) その他（下記に記載）

→ ユーザー回答:

**質問3:** 一括読み込みのタイミング:

- [ ] A) ファイルD&D時に全ページを即座に読み込み（待機時間あり）
- [ ] B) 最初のページ表示後、バックグラウンドで残りを読み込み
- [x] C) 両方選択可能（設定で切り替え）

→ ユーザー回答: C

---

### Q75. P119: 追加確認事項

**質問1:** TXT/MDファイルについて:
現在TXT/MDは全体を1画像として生成しています。これをどうしますか？

- [ ] A) 現状維持（1画像として一括生成）
- [x] B) ページ分割対応（長いファイルを複数ページに分割）
- [ ] C) その他

→ ユーザー回答:

**質問2:** 読み込みモード切替設定の配置場所:

- [ ] A) 設定タブに追加（全ファイル共通設定）
- [ ] B) ドキュメント翻訳画面のヘッダーにトグル追加
- [x] C) 両方（設定でデフォルト、画面で一時的に切替可能）

→ ユーザー回答:

**質問3:** 大きなファイル（100ページ以上）の場合:

- [ ] A) 警告なしで一括読み込み
- [x] B) 警告表示して続行確認
- [ ] C) 自動的にバックグラウンドモードに切替

→ ユーザー回答: B

---

### Q76. P119: TXT/MDページ分割の詳細

**質問1:** TXT/MDのページ分割基準:

- [ ] A) 固定行数（例: 50行ごと）
- [ ] B) 固定高さ（例: 1080pxごと、解像度設定に連動）
- [ ] C) 固定文字数（例: 2000文字ごと）
- [x] D) その他

→ ユーザー回答:A4サイズで余白狭くして縮尺100%。

**質問2:** 分割時の区切り処理:

- [ ] A) 行の途中でも分割（単純な高さ基準）
- [x] B) 段落区切り（空行）で分割を優先
- [ ] C) 文の終わり（。や.）で分割を優先

→ ユーザー回答: B

---

### P119 実装計画（質疑応答まとめ）

**対象ファイル形式:**
- PDF: PyMuPDF一括レンダリング → ファイルキャッシュ
- Word: COM→PDF変換→一括レンダリング → ファイルキャッシュ
- Excel: 全シート一括CopyPicture → ファイルキャッシュ
- TXT/MD: A4サイズ（余白狭め）でページ分割 → ファイルキャッシュ
- PPT: ✅ 既存実装（Q70/Q73）

**TXT/MDページ分割仕様:**
- サイズ: A4 (210mm x 297mm = 794 x 1123 px @96dpi)
- 余白: 狭め（上下左右10mm程度）
- 縮尺: 100%
- 分割優先: 段落区切り（空行）

**読み込みモード:**
- 設定タブ: デフォルトモード選択（即時/バックグラウンド）
- ドキュメント画面: 一時的に切替可能なトグル

**大きなファイル（100ページ以上）:**
- 警告ダイアログ表示 → 続行/キャンセル選択

**他に質問はありません。** 質疑応答完了後、実装を開始します。

---

### P120. NLLB 3.3B ベンチマーク結果

**テスト条件:**
- ファイル: JP02.docx（52領域、1579文字）
- モデル: NLLB 3.3B (INT8量子化)
- 言語: 日本語 → 英語

**結果:**

| パターン | ロード場所 | 処理 | ロード時間 | 翻訳時間 |
|---------|-----------|------|-----------|---------|
| ① | VRAM | GPU | 4.3s | **47.3s** |
| ② | RAM | GPU | - | N/A (CTranslate2非対応) |
| ③ | RAM | CPU | 4.3s | **871.0s** |

**結論:** CPU は GPU の **18.4倍遅い**

---

### Q77. NLLB 3.3B 高速化方式

**ユーザー選択:**
- NLLB-3.3B: **Transformers + FP16 + device_map="auto" + torch.compile**
- NLLB-1.3B: 現状維持（CTranslate2）

**理由:**
- device_map="auto" で RAM+GPU 自動分割が可能
- torch.compile で追加高速化
- 3.3Bモデルのみ変更（1.3Bは既存の高速なCTranslate2を維持）

---

### Q78. P120: Windows共有GPUメモリの調査

**ユーザー質問:**
Windows共有GPUメモリ（11.8GB）がCUDA計算に使えないのはなぜ？

**Claude調査結果:**

| メモリ種別 | 用途 | CUDA利用 |
|-----------|------|----------|
| 専用VRAM | GPU計算、AIモデル | ✅ 可能 |
| 共有GPUメモリ | Windows UI、動画再生、ブラウザ | ❌ 不可 |

**理由:**
- 共有GPUメモリはPCIe経由でアクセス（~16GB/s）
- 専用VRAMは直接アクセス（~400GB/s）→ 25倍高速
- CUDAは専用VRAMを前提に設計されている

**結論:** Windows共有GPUメモリはディスプレイ/グラフィックス用であり、AI計算には使用不可

---

### Q79. P120: NLLB 3.3B 最適化オプション

**ユーザー質問:**
翻訳の流れをこれ以上高速化最適化できない？

**Claude提案（現在のメモリ状態での適用可能性）:**

| リソース | 使用量 | 余裕 |
|----------|--------|------|
| 専用VRAM | 3.8/4.0GB | 0.2GB（限界） |
| 共有GPU | 0.8/11.8GB | N/A（CUDA不可） |
| RAM | 14.4/23.6GB | 9GB（余裕あり） |

| 最適化 | 可否 | 理由 |
|--------|------|------|
| GPU層を増やす | ❌ | VRAMが限界 |
| **torch.compile** | ✅ | メモリ増加なしで高速化 |
| **ピンドメモリ** | ✅ | RAM余裕あり |
| **カスタムdevice_map** | ✅ | Decoder優先配置で転送削減 |
| Flash Attention | ⚠️ | Windows対応困難 |

**ユーザー選択:**
- [x] torch.compile
- [x] ピンドメモリ
- [x] カスタムdevice_map（最適配置）

---

### Q80. P120: カスタムdevice_map最適配置

**現状（device_map="auto"）の問題:**
```
メモリ順に配置 → Encoder優先でGPUに載る
GPU (3GB):  Embedding → Encoder層0-20
CPU (4GB):  Encoder層21-23 → Decoder全24層 → lm_head
→ Decoderが全てCPUにある → 毎トークン生成でCPU↔GPU転送が大量発生
```

**最適配置（Decoder優先）:**
```
GPU (3GB):  Embedding (1GB) + Decoder後半12層 (1.2GB) + lm_head
CPU (4GB):  Encoder全24層 + Decoder前半12層
→ 毎トークン生成で使われるDecoder/lm_headがGPUにある
```

**期待効果:**

| 指標 | 現在 | 最適化後 |
|------|------|----------|
| COPY使用率 | 40% | 20-25% |
| 翻訳速度 | 基準 | **30-50%高速化** |
| VRAM使用 | ~3GB | 変わらず |

---

## 追加質問（セッション4 - 質疑応答25）

---

### P122. 翻訳モデル同時読み込み問題

**ユーザー報告:**
翻訳モデルが複数同時に読み込まれてしまう。
- 一度に読み込む翻訳モデルは一つまで
- 翻訳には既にロードされているモデルを使用する事

**Q82-1:** 同時に読み込まれているのはどのモデル？

- [ ] A) NLLB 1.3B と 3.3B が両方
- [ ] B) Whisper と NLLB が両方
- [ ] C) その他（下記に記載）

→ ユーザー回答:具体的には分からないが、ドキュメント翻訳用に3.3Bが読み込まれた状態で、テキスト翻訳をNLLBで始めようとしたら、さらに大きな何かがGPUメモリ上にロードされた。

**Claude調査中...**

**Q82-2:** モデル切り替え時の期待動作は？

- [x] A) 新しいモデルをロードする前に、古いモデルを自動解放
- [ ] B) ユーザーに確認してから切り替え
- [ ] C) その他（下記に記載）

→ ユーザー回答:

---

### P123. キャッシュ翻訳の統一

**ユーザー報告:**
5つの翻訳機能（テキスト翻訳、音声認識、画像OCR、同時翻訳、ドキュメント翻訳）で、キャッシュ翻訳機能の活用状況が異なる。

例: ドキュメント翻訳での翻訳履歴はドキュメント翻訳のキャッシュに使われるが、テキスト翻訳時にはキャッシュに活用されていない。

**現状調査（Claude）:**

| 機能 | 翻訳経路 | キャッシュ |
|------|----------|-----------|
| テキスト翻訳 | Vue→Rust translate→Python translate() | ❓ 要確認 |
| 音声認識 | Vue→Rust→Python diarization | ❓ 要確認 |
| 画像OCR | Vue→Rust→Python ocr | ❓ 要確認 |
| 同時翻訳 | Vue→Rust→Python diarization | ❓ 要確認 |
| ドキュメント翻訳 | Vue→Rust batch_translate_document | ✅ Rustキャッシュ(state.cache) |

**Q83-1:** 期待するキャッシュ共有範囲は？

- [x] A) 全5機能で同一キャッシュを共有（テキスト翻訳の結果もドキュメント翻訳で使える）
- [ ] B) 各機能は独立キャッシュだが、全機能でキャッシュ機能を有効化
- [ ] C) その他（下記に記載）

→ ユーザー回答:

**Q83-2:** 現在キャッシュが効いていない（または不明な）機能はどれ？（複数選択可）

- [ ] A) テキスト翻訳
- [ ] B) 音声認識
- [ ] C) 画像OCR
- [ ] D) 同時翻訳
- [x] E) 全部確認してほしい

→ ユーザー回答:

---

### Claude調査結果（P122/P123）

#### P123: キャッシュ使用状況調査結果

| 機能 | Rustファイル | キャッシュ使用 | 詳細 |
|------|-------------|---------------|------|
| テキスト翻訳 | translation.rs | ✅ | `state.cache.get()` / `state.cache.insert()` |
| ドキュメント翻訳 | document.rs | ✅ | `state.cache.get_with_api()` / `state.cache.insert()` |
| 音声認識 | audio.rs | ❌ 未実装 | キャッシュ関連コードなし |
| 画像OCR | ocr.rs | ❌ 未実装 | キャッシュ関連コードなし |
| 同時翻訳 | audio.rs | ❌ 未実装 | キャッシュ関連コードなし |

**結論:**
- テキスト翻訳とドキュメント翻訳: ✅ 同一のRustキャッシュ（`state.cache`）を共有
- 音声認識/画像OCR/同時翻訳: ❌ キャッシュ未実装

**修正方針:**
1. `audio.rs`: 翻訳結果を`state.cache`に保存/取得するコードを追加
2. `ocr.rs`: 翻訳結果を`state.cache`に保存/取得するコードを追加

---

#### P122: モデル同時読み込み問題（原因特定）

**原因特定:**

1. **問題箇所:** `translation.py`の`set_nllb_model_size()`関数（326-348行目）
2. **問題点:** モデルサイズ変更時、参照（`_ct2_model = None`, `_hf_model = None`）をクリアするだけで、`torch.cuda.empty_cache()`を呼んでいない
3. **結果:** Python参照はNoneになるが、GPUメモリは解放されない → 新しいモデルをロードすると両方がGPUに存在

**現在のコード（問題あり）:**
```python
def set_nllb_model_size(size: str) -> None:
    if size != _current_model_size:
        _current_model_size = size
        _model_loaded = False
        _ct2_model = None      # ← 参照クリアのみ
        _sp_model = None
        _hf_model = None       # ← 参照クリアのみ
        _hf_tokenizer = None
        _hf_model_loaded = False
        # ↑ torch.cuda.empty_cache() が呼ばれていない！
```

**修正方針:**
```python
def set_nllb_model_size(size: str) -> None:
    if size != _current_model_size:
        # P122: 既存モデルをGPUから解放してから新しいサイズを設定
        if _model_loaded or _hf_model_loaded:
            release_gpu_memory()  # これでGPUメモリも解放される
        _current_model_size = size
        print(f"[NLLB] Model size set to {size}")
```

**追加修正（document.rs）:**
- `batch_translate_document`コマンドでも翻訳前に`set_nllb_model_size()`を呼び出す
- 現在は`translation.rs`のみで設定、`document.rs`では設定していない

---

### Q84. P122続: 3.3Bモデルが解放できない問題

**ユーザー報告:**
まだ一度読み込んだ3.3Bモデルが、ドキュメント翻訳後に解放できなくなる。

**確認質問:**

**Q84-1:** 「解放できない」の詳細は？

- [ ] A) サイドバーの「解放」ボタンを押しても反応しない
- [ ] B) 「解放」ボタンを押すとUIは変わるがVRAMは減らない
- [x] C) 「解放」ボタンがグレーアウトして押せない
- [ ] D) その他（下記に記載）

→ ユーザー回答:1.3Bを新規に追加読込は出来るが、その後「解放」ボタン押下で解放されるのは1.3Bだけ。ロードされた3.3Bは残ったまま。

**Q84-2:** 問題が発生する手順は？

- [ ] A) 3.3Bロード → ドキュメント翻訳 → 解放ボタン押下 → 解放失敗
- [ ] B) 3.3Bロード → テキスト翻訳 → 解放ボタン押下 → 解放失敗
- [ ] C) アプリ起動直後から3.3Bがロード済みで解放できない
- [x] D) その他（下記に記載）

→ ユーザー回答:3.3Bロード(VRAM3.8GB占有) → ドキュメント翻訳 →VRAM占有が2.5GBに落ちる→テキスト翻訳 → VRAM3.8GB + 共有メモリ2.5GBまで増える→解放ボタン押下→VRAM占有が2.5GBに落ちる→これ以上は落ちない。

---

### Q85. P123続: キャッシュがテキスト翻訳で共有されない問題

**ユーザー報告:**
ドキュメント翻訳からのオフライン翻訳結果が「翻訳履歴」や「対訳比較」タブに存在するのに、テキスト翻訳タブで同じ原文をオフライン翻訳するとキャッシュヒットしない。一度テキスト翻訳タブで翻訳すると、次からはキャッシュヒットする。

**確認質問:**

**Q85-1:** 「翻訳履歴」「対訳比較」タブのデータソースは？

- [ ] A) Rustのstate.cacheから取得
- [ ] B) localStorageから取得
- [ ] C) 両方
- [x] D) わからない

→ ユーザー回答:同じドキュメント翻訳からの翻訳履歴であっても、一度GUIを閉じて、再度立ち上げると、翻訳履歴には同じ内容が残っているように見えるのに、ドキュメント翻訳でもキャッシュヒットしなくなる。

**Q85-2:** ドキュメント翻訳時のAPIは？

- [x] A) offline（NLLB）
- [ ] B) DeepL
- [ ] C) Google
- [ ] D) その他

→ ユーザー回答:

---

### Claude分析結果（Q84/Q85回答から）

#### P122分析: 3.3Bと1.3Bが別々にロードされている

**ユーザー報告の流れ:**
```
3.3Bロード (VRAM 3.8GB)
    ↓
ドキュメント翻訳
    ↓
VRAM 2.5GBに減少 ← ★ここで何かが解放されている
    ↓
テキスト翻訳
    ↓
VRAM 3.8GB + 共有メモリ 2.5GB ← ★1.3Bが追加ロードされた？
    ↓
解放ボタン押下
    ↓
VRAM 2.5GBに戻る ← ★1.3Bのみ解放、3.3Bは残る
```

**推定原因:**
- ドキュメント翻訳とテキスト翻訳で異なるモデルサイズ設定が使われている
- `document.rs`が3.3Bを使用、`translation.rs`が1.3Bを使用している可能性

---

#### P123分析: キャッシュがアプリ再起動で消える

**重要発見:**
「一度GUIを閉じて再起動すると、翻訳履歴は残っているのにキャッシュヒットしなくなる」

**原因:**
- 翻訳履歴: `localStorage`に保存 → **永続化**
- Rustキャッシュ: `state.cache`はメモリ内 → **揮発性**

**解決策:**
- キャッシュを`localStorage`または`SQLite`に永続化する必要あり

---

### P124. ドキュメント翻訳中のキャッシュヒット率表示

**ユーザー要望:**
ドキュメント翻訳中に「その時点で何％キャッシュヒットしているか」も進捗表示して。

**Q86-1:** 表示形式の希望は？

- [ ] A) 進捗バーの横に「キャッシュヒット: 45%」と表示
- [x] B) 進捗バーの下に別行で表示
- [x] C) 進捗バー自体を2色で表示（翻訳済み=青、キャッシュヒット=緑）
- [ ] D) その他（下記に記載）

→ ユーザー回答:

---

### P125. キャッシュ検索を翻訳モデル読み込み前に完了

**ユーザー要望:**
キャッシュ検索は翻訳モデル読み込み前に完了させて。

**Q87-1:** 期待する動作は？

- [ ] A) 全テキストをキャッシュ検索 → キャッシュミスがあればモデルロード → 翻訳実行
- [ ] B) 全テキストをキャッシュ検索 → 100%ヒットならモデルロードをスキップ
- [x] C) 両方（A + B）
- [ ] D) その他

→ ユーザー回答:

**Q87-2:** キャッシュ検索中の進捗表示は必要？

- [x] A) はい（「キャッシュ検索中... 50/100」など）
- [ ] B) いいえ（高速なので不要）

→ ユーザー回答:

---

### Q88. P123: キャッシュ永続化方式

**分析結果から:**
アプリ再起動でキャッシュが消えるのは、Rustの`state.cache`がメモリ内のみだから。

**Q88-1:** キャッシュ永続化の保存先は？

- [ ] A) SQLite（既存のデータベースに追加）
- [ ] B) localStorage（Vue側で管理）
- [ ] C) 専用ファイル（JSON/バイナリ）
- [x] D) おまかせ

→ ユーザー回答:キャッシュ翻訳を最速化・ファイルサイズ最小化・最高効率化するにはどうしたらいい？翻訳履歴や対訳比較タブと連携・統合させて、省ファイルサイズ化出来る？Claudeが考える最も良い方法は？

**Q88-2:** キャッシュの有効期限は？

- [x] A) 無期限（手動クリアまで保持）
- [ ] B) 1週間
- [ ] C) 1ヶ月
- [ ] D) 設定で選択可能に

→ ユーザー回答:

**Q88-3:** キャッシュサイズ上限は？

- [x] A) 無制限
- [ ] B) 1000件
- [ ] C) 10000件
- [ ] D) 設定で選択可能に

→ ユーザー回答:

---

### Claude提案: 最適キャッシュ設計

**ユーザーの質問:**
キャッシュ翻訳を最速化・ファイルサイズ最小化・最高効率化するには？翻訳履歴や対訳比較タブと連携・統合させて省ファイルサイズ化できる？

---

#### 推奨案: SQLite統合キャッシュ

**現状の問題:**
```
翻訳履歴:     localStorage (Vue)  ← 5-10MB制限、重複保存
対訳比較:     localStorage (Vue)  ← 同上
Rustキャッシュ: state.cache (メモリ) ← 揮発性、再起動で消える
```

**提案する構成:**
```
SQLite (1ファイル、制限なし)
  └── translation_cache テーブル
        ├── 原文ハッシュ (PRIMARY KEY) ← 高速検索
        ├── 原文、訳文、言語ペア、API種別
        └── タイムスタンプ
```

---

#### メリット

| 項目 | 効果 |
|------|------|
| **最速化** | B-treeインデックスでO(log n)検索 |
| **ファイルサイズ最小化** | 原文ハッシュで重複排除、同じ文は1回だけ保存 |
| **翻訳履歴統合** | 履歴はキャッシュIDを参照するだけ（データ重複なし） |
| **永続化** | アプリ再起動後もキャッシュ有効 |
| **無制限** | localStorageの5-10MB制限なし |

---

#### テーブル設計

```sql
-- キャッシュ本体（翻訳結果を1回だけ保存）
CREATE TABLE translation_cache (
    source_hash TEXT PRIMARY KEY,  -- SHA256先頭16文字
    source_text TEXT NOT NULL,
    target_text TEXT NOT NULL,
    source_lang TEXT NOT NULL,
    target_lang TEXT NOT NULL,
    api TEXT NOT NULL,  -- 'offline', 'deepl', 'google'
    created_at INTEGER DEFAULT (strftime('%s','now'))
);

-- 翻訳履歴（キャッシュを参照、データ重複なし）
CREATE TABLE translation_history (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    cache_hash TEXT NOT NULL,  -- translation_cache.source_hash
    context TEXT NOT NULL,     -- 'text', 'document', 'audio', 'ocr'
    created_at INTEGER DEFAULT (strftime('%s','now')),
    FOREIGN KEY (cache_hash) REFERENCES translation_cache(source_hash)
);
```

---

#### 効率化の例

**従来（localStorage）:**
```
翻訳履歴: "Hello" → "こんにちは" (50バイト)
対訳比較: "Hello" → "こんにちは" (50バイト) ← 重複！
キャッシュ: "Hello" → "こんにちは" (50バイト) ← 重複！
合計: 150バイト
```

**提案（SQLite統合）:**
```
translation_cache: "Hello" → "こんにちは" (50バイト)
translation_history: cache_hash参照 (16バイト)
合計: 66バイト (56%削減)
```

---

**Q89:** この設計で進めてよいですか？

- [x] A) はい、この設計で実装
- [ ] B) 翻訳履歴/対訳比較はlocalStorageのまま、キャッシュのみSQLite
- [ ] C) 別の方法を検討

→ ユーザー回答:日中韓の発音記号連携機能との統合はどうする？

---

### Claude提案: 発音記号（ローマ字化）の統合

**現在の発音記号機能:**
- 中国語: pypinyin（ピンイン）
- 日本語: pykakasi（ローマ字）
- 韓国語: hangul-romanize（ローマ字）

---

#### 案A: 翻訳キャッシュに列追加（推奨）

```sql
CREATE TABLE translation_cache (
    source_hash TEXT PRIMARY KEY,
    source_text TEXT NOT NULL,
    target_text TEXT NOT NULL,
    source_lang TEXT NOT NULL,
    target_lang TEXT NOT NULL,
    api TEXT NOT NULL,
    source_romanization TEXT,  -- 原文の発音記号（日中韓のみ）
    target_romanization TEXT,  -- 訳文の発音記号（日中韓のみ）
    created_at INTEGER
);
```

**メリット:**
- 翻訳と発音記号を1回のクエリで取得
- 構造がシンプル

**デメリット:**
- 同じテキストでも翻訳先言語が違えば発音記号が重複保存される

---

#### 案B: 発音記号を別テーブル

```sql
-- 発音記号専用テーブル
CREATE TABLE romanization_cache (
    text_hash TEXT PRIMARY KEY,  -- テキストのハッシュ
    text TEXT NOT NULL,
    lang TEXT NOT NULL,          -- 'ja', 'zh', 'ko'
    romanization TEXT NOT NULL,
    created_at INTEGER
);
```

**メリット:**
- 同じテキストの発音記号は1回だけ保存（最小サイズ）
- 翻訳とは独立してキャッシュ可能

**デメリット:**
- 2テーブル参照が必要

---

#### 推奨: 案A（列追加）

理由:
1. 発音記号は軽量（元テキストより短い）
2. クエリ1回で全データ取得可能
3. 実装がシンプル

---

**Q90:** 発音記号の統合方式は？

- [ ] A) 案A（翻訳キャッシュに列追加）← 推奨
- [ ] B) 案B（発音記号を別テーブル）
- [x] C) 発音記号はキャッシュしない（毎回生成）

→ ユーザー回答:翻訳履歴タブ、対訳比較タブ共に、「設定」タブで発音記号を表示すると決定した言語は、原文でも訳文でも発音記号を表示する様に。

---

### P126. 翻訳履歴/対訳比較での発音記号表示

**ユーザー要望:**
- 設定タブで発音記号表示ONにした言語は、翻訳履歴・対訳比較でも表示
- 原文・訳文の両方に適用

**確認質問:**

**Q91-1:** 発音記号の表示位置は？

- [x] A) テキストの下に小さく表示（ルビ風）
- [ ] B) テキストの横に括弧で表示（例: こんにちは (konnichiwa)）
- [ ] C) ホバー/クリックで表示（ツールチップ）
- [ ] D) その他

→ ユーザー回答:

**Q91-2:** 現在の設定タブに発音記号ON/OFF設定は存在する？

- [x] A) はい、既にある
- [ ] B) いいえ、新規追加が必要

→ ユーザー回答:Claudeが自身で確認して。

---

### Claude調査結果: 発音記号設定の現状

**設定タブ (settings.ts):**
```typescript
show_romanization_ja: boolean  // 日本語ローマ字表示
show_romanization_ko: boolean  // 韓国語ローマ字表示
show_romanization_zh: boolean  // 中国語ピンイン表示
korean_romanization_format: 'RR' | 'MR'  // ローマ字形式
```
→ ✅ **設定は既に存在**

**翻訳履歴タブ (HistoryView.vue):**
- 原文・訳文のテキストのみ表示
- 発音記号の表示機能なし
→ ❌ **実装が必要**

**対訳比較タブ (ComparisonView.vue):**
- 翻訳時に`korean_romanization_format`を渡しているが
- 表示部分では発音記号未対応
→ ❌ **実装が必要**

---

**P126実装内容:**

| タブ | 現状 | 実装内容 |
|------|------|----------|
| 翻訳履歴 | 発音記号なし | 原文/訳文の下にルビ風表示を追加 |
| 対訳比較 | 発音記号なし | 原文/訳文の下にルビ風表示を追加 |

**表示条件:**
- `show_romanization_ja === true` かつ テキストが日本語 → ローマ字表示
- `show_romanization_ko === true` かつ テキストが韓国語 → ローマ字表示
- `show_romanization_zh === true` かつ テキストが中国語 → ピンイン表示

---

他に質問はありません。質疑応答完了でよろしいですか？

いいえ。韓国語はRR形式と、MR形式に両対応する事。設定タブで両方から選ぶことが出来ます。

---

## 追加質問（セッション5 - 質疑応答27）

---

### ユーザー報告: P122-P127 状況

**P122 問題残存:**
- NLLB 3.3Bロード後、解放ボタンを押しても2.5GB程度VRAMに残る
- 1分程度後、あらためて3.3Bがロードされ、再度「解放」ボタンが押せる様になる
- 解放ボタンを押しても2.5GB程度VRAMに残り、1分程度また3.3Bが勝手にロードされる
- これが繰り返される

**P123, P124:** ✅ 解決済み

**P125 問題残存:**
- まだ翻訳モデルがロード完了してからキャッシュを検索開始している

**P126 問題残存:**
- 翻訳履歴タブでも対訳比較タブでも、発音記号が生成・表示されない

**P127 新規:**
- ドキュメント翻訳中に「中止ボタン」を押しても、即座に中止されない

---

### Q92. P122: 解放ボタンと自動再ロード問題

**Q92-1:** 「解放」ボタンを押した時、どの画面で押していますか？

- [ ] A) テキスト翻訳タブの「解放」ボタン
- [ ] B) ドキュメント翻訳タブの「解放」ボタン
- [ ] C) 設定タブの「解放」ボタン
- [x] D) その他（下記に記載）

→ ユーザー回答:それぞれのタブに解放ボタンはありません。サイドバーだけに解放ボタンがあります。サイドバーから押しています。

**Q92-2:** 1分後に3.3Bが「勝手に」再ロードされる時、何か操作をしていますか？

- [ ] A) 完全に放置状態（何も操作していない）
- [x] B) 他のタブを表示している
- [ ] C) 翻訳操作を行っている
- [ ] D) その他（下記に記載）

→ ユーザー回答:

---

### Q93. P125: キャッシュ検索タイミング

**Q93-1:** ログで「[P125] Pre-scanning cache」が表示されていますか？

- [ ] A) はい、表示されているがモデルロード後に出る
- [x] B) いいえ、そのログは表示されない
- [ ] C) 確認していない/わからない

→ ユーザー回答:[NLLB] Model size set to 3.3B (loaded: None)
[P122] release_gpu_memory called (loaded_size=None, ct2=False, hf=False)
[P122] GPU memory released (allocated=0.02GB, reserved=2.04GB)

---

### Q94. P126: 発音記号が表示されない問題

**Q94-1:** 設定タブで発音記号表示（日本語/韓国語/中国語）はONになっていますか？

- [x] A) はい、ONになっている
- [ ] B) いいえ、OFFになっている
- [ ] C) 確認していない

→ ユーザー回答:

**Q94-2:** 翻訳履歴に日本語/韓国語/中国語への翻訳結果はありますか？

- [x] A) はい、ある
- [ ] B) いいえ、ない
- [ ] C) 確認していない

→ ユーザー回答:

---

### Q95. P127: 中止ボタンの即時性

**Q95-1:** 中止ボタンを押してから実際に止まるまで、どのくらい時間がかかりますか？

- [ ] A) 1-5秒程度
- [ ] B) 10秒以上
- [x] C) 全く止まらない（最後まで実行される）
- [ ] D) その他（下記に記載）

→ ユーザー回答:

**Q95-2:** 「即座に」とは、どの程度の速さを期待していますか？

- [ ] A) 1秒以内
- [x] B) 5秒以内
- [x] C) 現在処理中の1文が終わったら停止
- [ ] D) その他（下記に記載）

→ ユーザー回答:

---

## 追加質問（セッション6 - P130フォローアップ）

---

### Q96. P130: VRAM予約モデルの変更

**ユーザー要望:**
- 1.3Bはほぼ使わない
- 1.3Bはロードに時間もそんなにかからないので、3.3Bの予約の方がいい
- 3.3Bの予約分も完全にアンロードする機能を付けて

**Q96-1:** 3.3B予約の条件は？

- [x] A) 3.3Bを使用した後は常に3.3BをVRAMに残す
- [ ] B) 設定で「デフォルトモデル」を選択できるようにする（1.3B/3.3B）
- [ ] C) その他（下記に記載）

→ ユーザー回答:

**Q96-2:** 1.3B使用後の動作は？

- [x] A) 1.3B使用後は即座に解放（VRAMに残さない）
- [ ] B) 1.3B使用後も一時的にVRAMに残す（次回3.3B使用時に解放）
- [ ] C) その他（下記に記載）

→ ユーザー回答:

**Q96-3:** 完全アンロード機能のUI配置は？

- [ ] A) サイドバーの既存「解放」ボタンを強化（長押しで完全解放など）
- [ ] B) 設定タブに「VRAM完全解放」ボタンを追加
- [x] C) サイドバーに別途「完全解放」ボタンを追加
- [ ] D) その他（下記に記載）

→ ユーザー回答:

---

### Q97. P130: 完全解放ボタンの詳細

**Q97-1:** 「完全解放」ボタンのラベル/アイコンは？

- [ ] A) 「完全解放」テキスト
- [x] B) 「VRAM解放」テキスト
- [ ] C) アイコンのみ（ゴミ箱/メモリ解放等）
- [ ] D) その他（下記に記載）

→ ユーザー回答:

**Q97-2:** 既存の「解放」ボタンとの違いを明確にするため、既存ボタンのラベルを変更しますか？

- [ ] A) 現状維持（「解放」のまま）
- [ ] B) 「一時解放」に変更
- [ ] C) 「モデル解放」に変更
- [ ] D) その他（下記に記載）

→ ユーザー回答:完全解放ボタンのみで良い。既存ボタンの機能を完全解放に変更。

**Q97-3:** 完全解放ボタンの配置位置は？

- [ ] A) 既存「解放」ボタンの下
- [ ] B) 既存「解放」ボタンの横
- [ ] C) NLLBセクションの最下部
- [ ] D) その他（下記に記載）

→ ユーザー回答:既存ボタンの置き換え。

---

### P130 実装計画（質疑応答まとめ）

**要件:**
1. 3.3B使用後は常にVRAMに残す（予約）
2. 1.3B使用後は即座に解放（VRAMに残さない）
3. 既存「解放」ボタンを「VRAM解放」に変更し、完全解放機能に置き換え

**実装内容:**

| 修正箇所 | 内容 |
|----------|------|
| `translation.py` | `release_gpu_memory()`を強化、予約メモリも含めて完全解放 |
| `translation.py` | 1.3B使用後の自動解放ロジック追加 |
| `translation.py` | 3.3B使用後はVRAMに残す（解放しない）ロジック追加 |
| サイドバーVue | 「解放」→「VRAM解放」にラベル変更 |
| ロケール4言語 | ボタンラベル翻訳追加 |

**動作フロー:**

```
3.3B使用 → 翻訳完了 → VRAMに残す（予約）
1.3B使用 → 翻訳完了 → 即座に解放
「VRAM解放」ボタン押下 → 3.3B予約分も含めて完全解放
```

---

他に質問はありません。質疑応答完了後、実装を開始します。

---

## P130 問題残存（2025-12-31）

**ユーザー報告:**
1. 1.3Bをロードした後に翻訳を始めると、3.3Bが追加ロードされてしまう
2. 1.3Bがロードされている場合はオフライン翻訳は1.3Bを使う事
3. 3.3Bがロードされている場合はオフライン翻訳は3.3Bを使う事
4. VRAM解放ボタンを押してもまだ2.5GBのVRAM占有が残る

**根本原因（調査中）:**
- `_current_model_size`と`_loaded_model_size`が同期していない可能性
- モデルサイズ設定と実際にロードされているモデルの不一致

---

## P131: テキスト翻訳のオンライン/オフライン並列表示

**ユーザー要望:**
「テキスト翻訳」タブでオンライン翻訳をする場合、表示する優先順位:
1. キャッシュ翻訳
2. オフライン翻訳かオンライン翻訳の先に翻訳完了した方
3. ②でオフライン翻訳結果が先に表示された場合のみ、オンライン翻訳結果で上書き

**実装方針:**
- オンライン翻訳時、同時にオフライン翻訳も並列実行
- 先に完了した方を表示
- オフラインが先の場合、オンライン完了時に上書き

---

## P130残存（2025-12-31 続報）

**ユーザー追加報告:**
1. 現在ロードされているのが1.3B/3.3Bどちらかわかるようにサイドバーに表示して
2. VRAM内容を常に監視し、ロードされている方を表示
3. まだVRAM解放ボタンを押しても2.5GBがVRAM上に残存している
4. VRAM解放ボタンはいつでも押せるようにして（現在はグレーアウト？）

---

### Q98. P130残: サイドバー表示詳細

**Q98-1:** モデル状態の表示形式は？

- [ ] A) 「現在: 1.3B」または「現在: 3.3B」のテキスト表示
- [ ] B) 「1.3B [ロード済]」「3.3B [未ロード]」のような状態表示
- [x] C) ロードボタンの色/状態で示す（ロード済みのボタンが強調表示）
- [ ] D) その他（下記に記載）

→ ユーザー回答:

**Q98-2:** VRAM使用量のリアルタイム表示は必要？

- [ ] A) はい、VRAM使用量(GB)をリアルタイムで表示
- [ ] B) いいえ、モデル名（1.3B/3.3B/なし）のみ表示
- [x] C) 両方表示
- [ ] D) その他（下記に記載）

→ ユーザー回答:

---

## P131確認

**ユーザー質問:**
オフライン翻訳よりもオンライン翻訳結果が先に帰ってきた場合、オフライン翻訳結果を待たずにオンライン翻訳結果を表示させるようになっている？

**Claude回答:**
はい、実装済みです。`Promise.race`を使用して、先に完了した方（オフラインでもオンラインでも）を即座に表示します。

**動作パターン:**
1. **オフラインが先に完了**: オフライン結果を表示 → オンライン完了後に上書き
2. **オンラインが先に完了**: オンライン結果を即座に表示 → オフライン結果は破棄
3. **キャッシュヒット**: キャッシュ結果を即座に表示（API呼び出しなし）

---

## P132: 翻訳時間表示（新規）

**ユーザー要望:**
1. テキスト翻訳実行時、何秒かかったかをAPI表示の右側に表示させて
2. 速度測定タブで測定すると300ms程度と表示されるのに、テキスト翻訳は体感は300ms以上かかる

---

### Q99. P132: 翻訳時間表示詳細

**Q99-1:** 表示形式は？

- [x] A) 「API: offline (300ms)」のようにms単位
- [ ] B) 「API: offline (0.3秒)」のように秒単位
- [ ] C) その他（下記に記載）

→ ユーザー回答:

**Q99-2:** 並列翻訳（P131）時の時間表示は？

- [ ] A) 最終結果（オンライン）の時間のみ表示
- [x] B) 両方表示（例: offline 300ms → google 500ms）
- [ ] C) 最初に表示された結果の時間を表示
- [ ] D) その他（下記に記載）

→ ユーザー回答:

---

**最終更新:** 2025-12-31
